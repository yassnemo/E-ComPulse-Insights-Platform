# E-ComPulse Insights Platform

## ðŸš€ Production-Grade Real-Time E-Commerce Analytics Platform

A scalable, enterprise-ready data platform designed to ingest, process, and analyze 100,000+ e-commerce events per minute with zero downtime and infinite scalability.

### ðŸ—ï¸ Architecture Overview

```mermaid
graph TB
    subgraph "Data Sources"
        WEB[Web Tracker JS]
        MOBILE[Mobile SDK]
        SYNTH[Synthetic Generator]
    end
    
    subgraph "AWS EKS Cluster"
        subgraph "Ingestion Layer"
            KAFKA[Apache Kafka MSK]
            PROXY[Kafka REST Proxy]
        end
        
        subgraph "Processing Layer"
            SPARK[Spark Structured Streaming]
            REDIS[Redis Cache]
        end
        
        subgraph "Orchestration"
            AIRFLOW[Apache Airflow]
            GE[Great Expectations]
        end
        
        subgraph "Monitoring"
            PROM[Prometheus]
            GRAF[Grafana]
        end
    end
    
    subgraph "Storage & Analytics"
        S3[S3 Data Lake]
        REDSHIFT[Amazon Redshift]
        TABLEAU[Tableau Server]
    end
    
    WEB --> PROXY
    MOBILE --> PROXY
    SYNTH --> KAFKA
    PROXY --> KAFKA
    KAFKA --> SPARK
    SPARK --> REDIS
    SPARK --> S3
    AIRFLOW --> SPARK
    AIRFLOW --> GE
    S3 --> REDSHIFT
    REDSHIFT --> TABLEAU
    
    SPARK --> PROM
    KAFKA --> PROM
    AIRFLOW --> PROM
    PROM --> GRAF
```

### ðŸŽ¯ Key Features

- **Infinite Scalability**: Auto-scaling at every layer with no single point of failure
- **Exactly-Once Processing**: Guaranteed data consistency with Spark checkpointing
- **Zero-Downtime Deployment**: Blue/green deployments with feature flag support
- **Real-Time Processing**: Sub-second event processing with Structured Streaming
- **Enterprise Security**: TLS encryption, IAM policies, network isolation
- **Cost Optimized**: Spot instances, auto-scaling, intelligent data retention

### ðŸ“ Project Structure

```
â”œâ”€â”€ infrastructure/          # Terraform IaC for AWS (VPC, EKS, MSK, RDS, Redis, S3)
â”œâ”€â”€ data-ingestion/          # Event generators and SDKs
â”‚   â”œâ”€â”€ synthetic-generator/ # Python event generator (200k events/min)
â”‚   â”œâ”€â”€ web-tracker/         # JavaScript web tracking SDK
â”‚   â””â”€â”€ mobile-sdk/          # Android/iOS native SDKs
â”œâ”€â”€ kafka-config/            # Kafka topics, producers, consumers
â”œâ”€â”€ spark-streaming/         # Scala Structured Streaming jobs
â”œâ”€â”€ airflow-dags/            # Apache Airflow DAGs with Great Expectations
â”œâ”€â”€ k8s/                     # Kubernetes manifests and Helm charts
â”‚   â”œâ”€â”€ helm/                # Production-ready Helm charts
â”‚   â””â”€â”€ manifests/           # Raw Kubernetes YAML files
â”œâ”€â”€ monitoring/              # Prometheus & Grafana observability
â”‚   â”œâ”€â”€ prometheus/          # Metrics collection and alerting
â”‚   â”œâ”€â”€ grafana/             # Dashboards and visualization
â”‚   â””â”€â”€ alertmanager/        # Alert routing and notifications
â”œâ”€â”€ tableau/                 # Business intelligence dashboards
â”‚   â”œâ”€â”€ workbooks/           # Tableau workbook files (.twbx)
â”‚   â”œâ”€â”€ data-sources/        # Data source connections
â”‚   â””â”€â”€ deployment/          # Automated deployment scripts
â”œâ”€â”€ scripts/                 # Deployment and utility scripts
â”œâ”€â”€ .github/workflows/       # CI/CD GitHub Actions pipelines
â””â”€â”€ docs/                    # Architecture & deployment guides
```

### ðŸš€ Installation & Setup Guide

This section provides comprehensive, step-by-step instructions to deploy the E-ComPulse Insights Platform from scratch. The platform supports multiple deployment environments (development, staging, production) with different resource configurations.

#### Prerequisites & System Requirements

##### Hardware Requirements
- **Development**: 16GB RAM, 4 CPU cores, 100GB storage
- **Production**: Distributed across AWS EKS with auto-scaling capabilities
- **Local Testing**: Docker Desktop with 8GB allocated memory

##### Required Software & Tools
```bash
# Core tools (macOS)
brew install terraform kubectl helm awscli docker
brew install jq yq git curl

# Core tools (Linux/Ubuntu)
sudo apt-get update
sudo apt-get install -y curl wget git unzip
# Follow official installation guides for: terraform, kubectl, helm, awscli, docker

# Core tools (Windows - using Chocolatey)
choco install terraform kubernetes-cli kubernetes-helm awscli docker-desktop git
```

##### Version Requirements
| Tool | Minimum Version | Recommended | Purpose |
|------|----------------|-------------|---------|
| Terraform | 1.5.0 | 1.6+ | Infrastructure as Code |
| kubectl | 1.27 | 1.28+ | Kubernetes management |
| Helm | 3.12 | 3.13+ | Package management |
| AWS CLI | 2.13 | 2.15+ | AWS resource management |
| Docker | 24.0 | 24.0+ | Containerization |

#### Step 1: Environment Preparation

##### 1.1 Clone Repository and Setup
```bash
# Clone the repository
git clone https://github.com/yassnemo/E-ComPulse-insights-platform.git
cd E-ComPulse-insights-platform

# Verify directory structure
ls -la
# Expected: infrastructure/, data-ingestion/, kafka-config/, spark-streaming/, etc.

# Make scripts executable
chmod +x scripts/*.sh
find . -name "*.sh" -exec chmod +x {} \;
```

##### 1.2 AWS Account Setup and Configuration
```bash
# Configure AWS credentials (use IAM user with programmatic access)
aws configure
# AWS Access Key ID: [your-access-key]
# AWS Secret Access Key: [your-secret-key]
# Default region name: us-west-2
# Default output format: json

# Verify AWS configuration
aws sts get-caller-identity
# Should return your account details

# Test required AWS services access
aws eks list-clusters --region us-west-2
aws kafka list-clusters --region us-west-2
aws s3 ls
```

##### 1.3 Required AWS IAM Permissions
Your AWS user/role needs the following permissions:
- **EC2**: Full access for VPC, Security Groups, Load Balancers
- **EKS**: Full access for cluster management
- **MSK**: Full access for Kafka cluster management
- **RDS**: Full access for database instances
- **ElastiCache**: Full access for Redis clusters
- **S3**: Full access for data lake storage
- **IAM**: Role creation and policy attachment
- **CloudWatch**: Logs and metrics access
- **Route53**: DNS management (if using custom domains)

#### Step 2: Infrastructure Deployment

##### 2.1 Terraform Backend Setup (Optional but Recommended)
```bash
# Create S3 bucket for Terraform state (one-time setup)
aws s3 mb s3://ecompulse-terraform-state-$(date +%s) --region us-west-2

# Create DynamoDB table for state locking
aws dynamodb create-table \
    --table-name ecompulse-terraform-locks \
    --attribute-definitions AttributeName=LockID,AttributeType=S \
    --key-schema AttributeName=LockID,KeyType=HASH \
    --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 \
    --region us-west-2

# Update backend.tf with your bucket name (optional)
```

##### 2.2 Environment Configuration
```bash
cd infrastructure

# Review and customize environment variables
cp environments/prod.tfvars environments/my-prod.tfvars
cp environments/dev.tfvars environments/my-dev.tfvars

# Edit configuration files
nano environments/my-prod.tfvars
```

**Key Configuration Variables:**
```hcl
# environments/my-prod.tfvars
region = "us-west-2"
environment = "production"
project_name = "ecompulse"

# VPC Configuration
vpc_cidr = "10.0.0.0/16"
availability_zones = ["us-west-2a", "us-west-2b", "us-west-2c"]

# EKS Configuration
eks_node_instance_types = ["m5.large", "m5.xlarge"]
eks_node_min_size = 3
eks_node_max_size = 20
eks_node_desired_size = 6

# MSK Configuration
kafka_instance_type = "kafka.m5.large"
kafka_broker_count = 3

# RDS Configuration
db_instance_class = "db.r5.large"
db_allocated_storage = 100
db_max_allocated_storage = 1000

# Redis Configuration
redis_node_type = "cache.r5.large"
redis_num_cache_nodes = 3

# Data Retention
s3_lifecycle_days = 90
logs_retention_days = 30

# Monitoring
enable_monitoring = true
enable_cloudtrail = true

# Cost Optimization
enable_spot_instances = true
spot_instance_percentage = 70
```

##### 2.3 Infrastructure Deployment
```bash
# Initialize Terraform
terraform init

# Validate configuration
terraform validate

# Plan deployment (review carefully)
terraform plan -var-file=environments/my-prod.tfvars -out=tfplan

# Deploy infrastructure (takes 15-25 minutes)
terraform apply tfplan

# Save important outputs
terraform output > ../terraform-outputs.txt
terraform output -json > ../terraform-outputs.json
```

##### 2.4 Post-Infrastructure Setup
```bash
# Update kubectl configuration
aws eks update-kubeconfig --region us-west-2 --name ecompulse-eks-cluster

# Verify cluster access
kubectl cluster-info
kubectl get nodes

# Install cluster autoscaler (if not included in Terraform)
kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml

# Verify infrastructure components
kubectl get all -A
```

#### Step 3: Application Deployment

##### 3.1 Pre-deployment Verification
```bash
# Verify all prerequisites
./scripts/verify-prerequisites.sh

# Check infrastructure status
kubectl get nodes
kubectl get ns

# Verify AWS resources
aws eks describe-cluster --name ecompulse-eks-cluster --region us-west-2
aws kafka list-clusters --region us-west-2
```

##### 3.2 Deploy Platform Components
```bash
# Deploy using master deployment script
./scripts/deploy.sh production

# Or deploy components individually:

# 1. Deploy Kafka configuration
cd kafka-config
./deploy-kafka-topics.sh
cd ..

# 2. Deploy Helm charts
cd k8s/helm
helm dependency update ecompulse-platform/
helm install ecompulse-platform ./ecompulse-platform/ \
    --namespace ecompulse \
    --create-namespace \
    --values ecompulse-platform/values-production.yaml \
    --timeout 15m

# 3. Deploy monitoring stack
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

helm install prometheus prometheus-community/kube-prometheus-stack \
    --namespace monitoring \
    --create-namespace \
    --values ../monitoring/prometheus/values.yaml

# 4. Deploy data ingestion components
kubectl apply -f ../manifests/data-ingestion/ -n ecompulse

# 5. Deploy Spark Streaming jobs
kubectl apply -f ../manifests/spark-streaming/ -n ecompulse

# 6. Deploy Airflow
helm repo add apache-airflow https://airflow.apache.org
helm install airflow apache-airflow/airflow \
    --namespace airflow \
    --create-namespace \
    --values ../manifests/airflow/values.yaml
```

##### 3.3 Deployment Verification
```bash
# Check all pods are running
kubectl get pods -A

# Verify services
kubectl get services -A

# Check persistent volumes
kubectl get pv

# Verify ingress controllers
kubectl get ingress -A

# Test internal connectivity
kubectl run test-pod --image=busybox -it --rm -- /bin/sh
# From within the pod, test DNS resolution:
# nslookup kafka-service.ecompulse.svc.cluster.local
```

#### Step 4: Access & Configuration

##### 4.1 Dashboard Access Setup
```bash
# Option 1: Port forwarding (for testing)
# Grafana
kubectl port-forward svc/grafana 3000:3000 -n monitoring &
echo "Grafana: http://localhost:3000 (admin/admin)"

# Prometheus
kubectl port-forward svc/prometheus 9090:9090 -n monitoring &
echo "Prometheus: http://localhost:9090"

# Kafka UI
kubectl port-forward svc/kafka-ui 8080:8080 -n ecompulse &
echo "Kafka UI: http://localhost:8080"

# Airflow
kubectl port-forward svc/airflow-webserver 8090:8080 -n airflow &
echo "Airflow: http://localhost:8090 (admin/admin)"

# Option 2: Load Balancer (for production)
kubectl get svc -A | grep LoadBalancer
# Use the external IPs provided
```

##### 4.2 Configure Data Sources and Connections
```bash
# Configure Kafka topics and verify
cd kafka-config
python3 verify_topics.py

# Test data ingestion
cd ../data-ingestion/synthetic-generator
pip3 install -r requirements.txt
python3 event_generator.py --mode test --duration 60

# Verify data flow in Kafka UI
# Navigate to http://localhost:8080 and check topic messages
```

##### 4.3 Import Grafana Dashboards
```bash
# Dashboards are automatically imported during Helm deployment
# Manually import if needed:
curl -X POST \
  http://admin:admin@localhost:3000/api/dashboards/db \
  -H 'Content-Type: application/json' \
  -d @monitoring/grafana/dashboards/platform-overview.json

curl -X POST \
  http://admin:admin@localhost:3000/api/dashboards/db \
  -H 'Content-Type: application/json' \
  -d @monitoring/grafana/dashboards/business-analytics.json
```

#### Step 5: Post-Deployment Validation

##### 5.1 End-to-End Testing
```bash
# Run comprehensive test suite
./scripts/run-tests.sh

# Test data pipeline
cd data-ingestion/synthetic-generator
python3 event_generator.py --mode load_test --events 10000 --rate 1000

# Verify data in dashboards
echo "Check Grafana dashboards for data flow visualization"
echo "Platform Overview: http://localhost:3000/d/platform-overview"
echo "Business Analytics: http://localhost:3000/d/business-analytics"

# Test Spark job execution
kubectl logs -f deployment/spark-event-enrichment -n ecompulse

# Verify Airflow DAG execution
# Navigate to Airflow UI and trigger data_quality_pipeline DAG
```

##### 5.2 Performance Validation
```bash
# Load testing with high event volume
cd data-ingestion/synthetic-generator
python3 event_generator.py --mode performance_test --events 100000 --rate 5000

# Monitor system metrics
kubectl top nodes
kubectl top pods -A

# Check auto-scaling behavior
kubectl get hpa -A
kubectl describe hpa ecompulse-synthetic-generator -n ecompulse
```

#### Troubleshooting Common Issues

##### Issue 1: EKS Cluster Creation Fails
```bash
# Check AWS quotas
aws service-quotas get-service-quota --service-code eks --quota-code L-1194D53C

# Verify IAM permissions
aws iam simulate-principal-policy \
    --policy-source-arn arn:aws:iam::ACCOUNT:user/USERNAME \
    --action-names eks:CreateCluster \
    --resource-arns "*"

# Clean up failed resources
terraform destroy -target=aws_eks_cluster.main
terraform apply -var-file=environments/my-prod.tfvars
```

##### Issue 2: Pod Scheduling Issues
```bash
# Check node capacity
kubectl describe nodes

# Check resource requests vs limits
kubectl describe pod POD_NAME -n NAMESPACE

# Scale cluster if needed
aws eks update-nodegroup-config \
    --cluster-name ecompulse-eks-cluster \
    --nodegroup-name ecompulse-worker-nodes \
    --scaling-config minSize=5,maxSize=30,desiredSize=10
```

##### Issue 3: Kafka Connection Issues
```bash
# Check MSK cluster status
aws kafka describe-cluster --cluster-arn $(aws kafka list-clusters --query 'ClusterInfoList[0].ClusterArn' --output text)

# Test connectivity from pod
kubectl run kafka-test --image=confluentinc/cp-kafka:latest -it --rm -- bash
# Inside pod:
# kafka-topics --bootstrap-server kafka-service:9092 --list

# Check security groups
aws ec2 describe-security-groups --group-ids $(terraform output kafka_security_group_id)
```

##### Issue 4: Monitoring Stack Issues
```bash
# Check Prometheus targets
kubectl port-forward svc/prometheus 9090:9090 -n monitoring
# Navigate to http://localhost:9090/targets

# Restart Grafana if dashboards not loading
kubectl rollout restart deployment/grafana -n monitoring

# Check PVC storage
kubectl get pvc -A
kubectl describe pvc prometheus-server -n monitoring
```

##### Issue 5: Network and DNS Issues
```bash
# Test cluster DNS
kubectl run test-dns --image=busybox -it --rm -- nslookup kubernetes.default.svc.cluster.local

# Check CoreDNS
kubectl get pods -n kube-system | grep coredns
kubectl logs -n kube-system deployment/coredns

# Verify VPC DNS settings
aws ec2 describe-vpcs --vpc-ids $(terraform output vpc_id) --query 'Vpcs[0].{DnsHostnames:DnsHostnames,DnsSupport:DnsSupport}'
```

#### Environment Variables Reference

Create a `.env` file in the project root with the following variables:

```bash
# AWS Configuration
AWS_REGION=us-west-2
AWS_ACCOUNT_ID=123456789012

# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=kafka-service:9092
KAFKA_SECURITY_PROTOCOL=PLAINTEXT
KAFKA_SASL_MECHANISM=PLAIN

# Redis Configuration  
REDIS_HOST=redis-service
REDIS_PORT=6379
REDIS_PASSWORD=your-secure-password

# Database Configuration
DB_HOST=postgres-service
DB_PORT=5432
DB_NAME=ecompulse
DB_USER=ecompulse_user
DB_PASSWORD=your-secure-password

# Monitoring
PROMETHEUS_URL=http://prometheus:9090
GRAFANA_URL=http://grafana:3000
GRAFANA_USER=admin
GRAFANA_PASSWORD=admin

# Application Configuration
LOG_LEVEL=INFO
METRICS_ENABLED=true
TRACING_ENABLED=true
FEATURE_FLAGS_ENABLED=true

# Spark Configuration
SPARK_MASTER=k8s://https://kubernetes.default.svc.cluster.local:443
SPARK_EXECUTOR_MEMORY=2g
SPARK_EXECUTOR_CORES=2
SPARK_DRIVER_MEMORY=1g

# Data Pipeline Configuration
CHECKPOINT_LOCATION=s3a://ecompulse-checkpoints/
OUTPUT_PATH=s3a://ecompulse-data-lake/
BATCH_INTERVAL=30s
WATERMARK_DELAY=1m
```

#### Next Steps After Installation

1. **Review Dashboards**: Access Grafana and explore the pre-built dashboards
2. **Configure Alerts**: Set up alert rules in `monitoring/prometheus/alerts.yaml`
3. **Customize Data Pipeline**: Modify Spark jobs in `spark-streaming/` for your use case
4. **Set Up Data Sources**: Configure connections to your existing data sources
5. **Performance Tuning**: Adjust resource limits and auto-scaling parameters
6. **Security Hardening**: Enable TLS, configure RBAC, and set up network policies
7. **Backup Strategy**: Configure automated backups for persistent data
8. **Documentation**: Update team runbooks and operational procedures

#### Support and Maintenance

- **Health Checks**: The platform includes automated health checks and self-healing
- **Updates**: Follow semantic versioning for component updates
- **Scaling**: Monitor auto-scaling metrics and adjust thresholds as needed
- **Cost Optimization**: Review AWS Cost Explorer recommendations monthly
- **Security**: Regular security scans and dependency updates via GitHub Actions

### ðŸ“Š Performance Targets

- **Throughput**: 100,000+ events/minute (tested up to 200k/min)
- **Latency**: <100ms end-to-end processing
- **Availability**: 99.9% uptime SLA with auto-failover
- **Scalability**: Auto-scale from 1k to 1M events/minute
- **Recovery**: <5 minute RTO, <1 minute RPO

### ðŸ” Security & Compliance

- **Encryption**: TLS 1.3 in transit, AES-256 at rest
- **Access Control**: IAM least-privilege, RBAC policies
- **Network Security**: VPC isolation, security groups, NACLs
- **Monitoring**: Comprehensive audit logging and alerting
- **Compliance**: GDPR/CCPA ready with data governance

### ðŸ—ï¸ Components Overview

#### Data Ingestion
- **Synthetic Generator**: 200k events/min with Redis caching and Prometheus metrics
- **Web Tracker SDK**: JavaScript library for React/Angular/vanilla HTML
- **Mobile SDKs**: Native Android (Kotlin) and iOS (Swift) with offline support

#### Stream Processing
- **Kafka**: MSK with 3-broker cluster, auto-scaling, monitoring
- **Spark Streaming**: Structured Streaming with exactly-once semantics
- **Redis**: ElastiCache for real-time caching and session management

#### Orchestration
- **Airflow**: Workflow management with Great Expectations data quality
- **Kubernetes**: EKS cluster with auto-scaling, monitoring, security

#### Monitoring & Analytics
- **Prometheus**: Metrics collection with 300+ custom metrics
- **Grafana**: 5 pre-built dashboards (Platform, Business, Kafka, Spark, Infrastructure)
- **Tableau**: Executive, operational, and customer analytics dashboards

### ðŸ“ˆ Monitoring Stack

#### Key Metrics
- **Platform**: Event rates, processing latency, error rates, throughput
- **Business**: Revenue, conversion rates, user behavior, product performance
- **Infrastructure**: CPU, memory, disk, network, Kubernetes resources
- **Kafka**: Consumer lag, broker health, topic throughput, partition distribution
- **Spark**: Job duration, records/batch, memory usage, scheduling delay

#### Alerting
- **Critical**: High error rates, system failures, data pipeline issues
- **Warning**: Resource usage, performance degradation, capacity limits
- **Info**: Deployments, scaling events, routine maintenance

### ðŸ”„ CI/CD Pipeline

#### GitHub Actions Workflow
- **Testing**: Unit tests, integration tests, security scans
- **Building**: Docker images with multi-stage builds, ECR push
- **Deployment**: Automated staging/production deployments
- **Monitoring**: Post-deployment health checks and notifications

#### Deployment Strategies
- **Blue/Green**: Zero-downtime production deployments
- **Canary**: Gradual rollout with traffic splitting
- **Feature Flags**: Safe feature releases with instant rollback

### ðŸ“ž Support & Documentation

#### Getting Help
- **Documentation**: Comprehensive READMEs in each component directory
- **Runbooks**: Step-by-step operational procedures in `/docs/runbooks/`
- **Troubleshooting**: Common issues and solutions in component READMEs
- **Architecture**: Detailed system design in `/docs/architecture/`

#### Key Documentation
- [Infrastructure Setup](infrastructure/README.md)
- [Data Ingestion Guide](data-ingestion/README.md)
- [Spark Streaming Jobs](spark-streaming/README.md)
- [Kubernetes Deployment](k8s/README.md)
- [Monitoring Setup](monitoring/README.md)
- [Tableau Analytics](tableau/README.md)

#### Utility Scripts

The platform includes several utility scripts to simplify deployment and maintenance:

```bash
# Verify all prerequisites before deployment
./scripts/verify-prerequisites.sh

# Complete platform deployment (development/staging/production)
./scripts/deploy.sh [environment]

# Run comprehensive test suite
./scripts/run-tests.sh

# Monitor platform health and performance
./scripts/health-check.sh

# Backup critical data and configurations
./scripts/backup.sh

# Update platform components
./scripts/update.sh [component]

# Clean up development/test resources
./scripts/cleanup.sh [environment]

# Generate sample data for testing
./scripts/generate-test-data.sh [volume] [duration]

# Scale platform components based on load
./scripts/scale.sh [component] [replicas]

# Export metrics and logs for analysis
./scripts/export-metrics.sh [timerange]
```
